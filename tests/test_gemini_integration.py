#!/usr/bin/env python3

import os
import asyncio
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

from whatsupdoc.config import Config
from whatsupdoc.vertex_search import VertexSearchClient
from whatsupdoc.gemini_rag import GeminiRAGService

def test_integration():
    print("ğŸ§ª Testing Gemini RAG Integration")
    print()
    
    # Load config
    config = Config()
    
    # Test configuration
    print("ğŸ” Testing configuration...")
    errors = config.validate()
    if errors:
        print(f"âŒ Configuration errors: {errors}")
        return False
    print("  âœ… Configuration valid")
    print(f"  ğŸ“ Gemini model: {config.gemini_model}")
    print(f"  ğŸ”§ Use Vertex AI: {config.use_vertex_ai}")
    print(f"  ğŸ›ï¸ Temperature: {config.answer_temperature}")
    print(f"  ğŸ“ Max context: {config.max_context_length}")
    print()
    
    # Initialize search client
    print("ğŸ” Testing search client...")
    search_client = VertexSearchClient(
        project_id=config.project_id,
        location=config.location,
        data_store_id=config.data_store_id,
        app_id=config.app_id
    )
    
    if not search_client.test_connection():
        print("âŒ Search client connection failed")
        return False
    print("  âœ… Search client connected")
    print()
    
    # Initialize Gemini RAG service
    print("ğŸ¤– Testing Gemini RAG service...")
    try:
        rag_service = GeminiRAGService(
            project_id=config.project_id,
            location=config.location,
            model=config.gemini_model,
            api_key=config.gemini_api_key if not config.use_vertex_ai else None,
            use_vertex_ai=config.use_vertex_ai,
            temperature=config.answer_temperature
        )
        
        if not rag_service.test_connection():
            print("âŒ Gemini connection failed")
            return False
        print("  âœ… Gemini service connected")
    except Exception as e:
        print(f"âŒ Failed to initialize Gemini service: {e}")
        return False
    print()
    
    # Test full RAG pipeline
    print("ğŸ”„ Testing full RAG pipeline...")
    test_query = "What are the main research methods mentioned in the documents?"
    
    async def test_rag_pipeline():
        # Search for documents
        print(f"  ğŸ” Searching for: '{test_query}'")
        results = await search_client.search(
            query=test_query,
            max_results=3,
            use_grounded_generation=True
        )
        print(f"  ğŸ“Š Found {len(results)} search results")
        
        if not results:
            print("  âš ï¸ No search results found - skipping RAG generation")
            return True
        
        # Generate RAG answer
        print("  ğŸ¤– Generating RAG answer...")
        rag_response = await rag_service.generate_answer(
            query=test_query,
            search_results=results,
            max_context_length=config.max_context_length
        )
        
        print(f"  ğŸ“ Generated answer ({len(rag_response.answer)} chars)")
        print(f"  ğŸ“Š Confidence: {rag_response.confidence_score:.1%}")
        print(f"  ğŸ”— Uses {len(rag_response.sources)} sources")
        print(f"  ğŸ“– Has citations: {rag_response.has_citations}")
        
        # Show a preview of the answer
        answer_preview = rag_response.answer[:200] + "..." if len(rag_response.answer) > 200 else rag_response.answer
        print(f"  ğŸ’¬ Answer preview: {answer_preview}")
        
        return True
    
    try:
        result = asyncio.run(test_rag_pipeline())
        if not result:
            return False
    except Exception as e:
        print(f"âŒ RAG pipeline test failed: {e}")
        return False
    
    print()
    print("=" * 50)
    print("ğŸ‰ All integration tests passed!")
    print("ğŸš€ Your Slack RAG bot with Gemini is ready!")
    print("=" * 50)
    return True

if __name__ == "__main__":
    success = test_integration()
    exit(0 if success else 1)